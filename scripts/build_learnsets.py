"""
Handles generating learnset codetables which are packed into NARC subfiles in codetables.mk

Responsible for:
  data/TMLearnsets.c
  data/LevelupLearnsets.c
  data/EggLearnsets.c
"""

import re
import json
import argparse

MAX_NUM_TMHMS = 128
TM_LEARNSETS_BITS_PER_WORD = 32
TM_LEARNSETS_BITFIELD_COUNT = MAX_NUM_TMHMS // TM_LEARNSETS_BITS_PER_WORD
MOVESET_TERMINATOR_32BIT = 0x0000FFFF
MOVESET_TERMINATOR_16BIT = 0xFFFF


def load_species_header(file_path):
    species_dict = {}
    index = 0
    with open(file_path) as f:
        for line in f:
            if len(line.split()) > 1:
                test = line.split()[1].strip()
                if 'SPECIES' in test and not '_START' in test and not '_SPECIES_H' in test and not '_NUM (' in line and not 'MAX_' in test:
                    species_dict[test] = index
                    index += 1
    return species_dict


def load_moves_header(file_path):
    moves_dict = {}
    index = 0
    with open(file_path) as f:
        for line in f:
            if len(line.split()) > 1:
                test = line.split()[1].strip()
                if 'MOVE' in test and not '_START' in test and not '_MOVES_H' in test and not 'NUM_OF' in test:
                    moves_dict[test] = index
                    index += 1
    return moves_dict


def load_tmhm_list(file_path):
    move_list = []
    in_array = False
    move_pattern = re.compile(r'\bMOVE_[A-Z0-9_]+')

    with open(file_path, encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if 'static const u16 sTMHMMoves[]' in line:
                in_array = True
                continue
            if in_array:
                if '};' in line:
                    break
                matches = move_pattern.findall(line)
                move_list.extend(matches)

    return move_list


def write_tm_data(species_dict, moves_dict, species_learnsets, tmhm_moves, output_path):
    tm_move_to_index = {
        move_name: idx
        for idx, move_name in enumerate(tmhm_moves)
        if move_name in moves_dict
    }

    max_species_index = max(species_dict.values())
    species_id_to_name = {v: k for k, v in species_dict.items()}

    with open(output_path, "w") as out:
        out.write("// DO NOT MODIFY THIS FILE!  autogenerated by build_learnsets.py\n\n")
        out.write("#include \"../include/types.h\"\n")
        out.write("#include \"../include/config.h\"\n")
        out.write("#include \"../include/constants/species.h\"\n")
        out.write("#include \"../include/constants/item.h\"\n\n")
        out.write(f"const u32 UNUSED TMLearnsets[][TM_LEARNSETS_BITFIELD_COUNT] = {{\n")

        for species_id in range(max_species_index + 1):
            species_name = species_id_to_name.get(species_id)
            learnset = []
            if species_name:
                learnset = species_learnsets.get(species_name, {}).get("TMMoves", [])
                learnset = list(set(m.strip() for m in learnset))

            parts = [0] * TM_LEARNSETS_BITFIELD_COUNT
            for move in learnset:
                tm_index = tm_move_to_index.get(move)
                if tm_index is not None and tm_index < MAX_NUM_TMHMS:
                    word = tm_index // TM_LEARNSETS_BITS_PER_WORD
                    bit = tm_index % TM_LEARNSETS_BITS_PER_WORD
                    parts[word] |= (1 << bit)

            formatted = ", ".join(f"0x{val:08X}" for val in parts)
            out.write(f"    [{species_name}] = {{ {formatted} }},\n")

        out.write("};\n")


def write_levelup_data(species_dict, moves_dict, species_learnsets, out_data_path, out_offset_path):
    species_id_to_name = {v: k for k, v in species_dict.items()}
    max_species_id = max(species_id_to_name.keys())

    flat_data = []
    species_blocks = []
    offsets = [0] * (max_species_id + 1)

    shared_terminator_offset = len(flat_data)
    flat_data.append(MOVESET_TERMINATOR_32BIT)

    for species_id in range(max_species_id + 1):
        species_name = species_id_to_name.get(species_id)
        learnset = []
        if species_name:
            learnset = species_learnsets.get(species_name, {}).get("LevelMoves", [])

        if learnset:
            start_index = len(flat_data)
            if species_name:
                species_blocks.append((start_index, species_name))

            for move_entry in learnset:
                move = move_entry.get("Move", "").strip()
                level = int(move_entry["Level"])
                if not move or move not in moves_dict:
                    print(f"[ERROR]: Invalid or missing move '{move}' for species '{species_name}' at level {level}")
                    exit(1)

                move_id = moves_dict[move]
                encoded = (level << 16) | move_id
                flat_data.append(encoded)

            flat_data.append(MOVESET_TERMINATOR_32BIT)
            offsets[species_id] = start_index
        else:
            offsets[species_id] = shared_terminator_offset

    species_block_map = {idx: name for idx, name in species_blocks}

    with open(out_data_path, "w") as out:
        out.write("// DO NOT MODIFY THIS FILE! Autogenerated by build_learnsets.py\n\n")
        out.write("#include \"../include/types.h\"\n")
        out.write("#include \"../include/config.h\"\n\n")
        out.write("const u32 UNUSED LevelUpLearnsets[] = {\n")

        for i, val in enumerate(flat_data):
            if i in species_block_map:
                out.write(f"    // {species_block_map[i]}\n")
            out.write(f"    0x{val:08X},\n")

        out.write("};\n")

    with open(out_offset_path, "w") as out:
        out.write("// DO NOT MODIFY THIS FILE! Autogenerated by build_learnsets.py\n\n")
        out.write("#include \"../include/types.h\"\n")
        out.write("#include \"../include/config.h\"\n\n")
        out.write("const u32 UNUSED LevelUpLearnsetOffsets[] = {\n")
        for species_id, offset in enumerate(offsets):
            species_name = species_id_to_name.get(species_id, "")
            comment = f" // {species_name}" if species_name else ""
            out.write(f"    {offset},{comment}\n")
        out.write("};\n")


def write_eggmove_data(species_dict, moves_dict, species_learnsets, out_data_path, out_offset_path):
    species_id_to_name = {v: k for k, v in species_dict.items()}
    max_species_id = max(species_id_to_name)

    flat_data = []
    offsets = [0] * (max_species_id + 1)
    species_offsets = []

    for species_id in range(max_species_id + 1):
        species_name = species_id_to_name.get(species_id)
        egg_moves = []
        if species_name:
            egg_moves = species_learnsets.get(species_name, {}).get("EggMoves", [])

        offsets[species_id] = len(flat_data)

        if species_name:
            species_offsets.append((len(flat_data), species_name))

        if egg_moves:
            for move in egg_moves:
                if move not in moves_dict:
                    print(f"[ERROR]: Move '{move}' not found in moves.h")
                    exit(1)
                move_id = moves_dict[move]
                flat_data.append(move_id)

        flat_data.append(MOVESET_TERMINATOR_16BIT)

    with open(out_data_path, "w") as f:
        f.write("// DO NOT MODIFY THIS FILE! Autogenerated by build_learnsets.py\n\n")
        f.write("#include \"../include/types.h\"\n")
        f.write("#include \"../include/config.h\"\n\n")
        f.write("const u16 UNUSED EggMoves[] = {\n")

        species_offset_map = {offset: name for offset, name in species_offsets}
        for i, move in enumerate(flat_data):
            if i in species_offset_map:
                f.write(f"    // {species_offset_map[i]}\n")
            f.write(f"    0x{move:04X},\n")

        f.write("};\n")

    with open(out_offset_path, "w") as f:
        f.write("// DO NOT MODIFY THIS FILE! Autogenerated by build_learnsets.py\n\n")
        f.write("#include \"../include/types.h\"\n")
        f.write("#include \"../include/config.h\"\n\n")
        f.write("const u32 UNUSED EggMoveOffsets[] = {\n")
        for species_id, offset in enumerate(offsets):
            species_name = species_id_to_name.get(species_id, "")
            comment = f" // {species_name}" if species_name else ""
            f.write(f"    {offset},{comment}\n")
        f.write("};\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--learnsets", default="data/mon/learnsets.json")

    parser.add_argument("--tmout")

    parser.add_argument("--levelupout")
    parser.add_argument("--levelupoffsetout")

    parser.add_argument("--eggout")
    parser.add_argument("--eggoffsetout")

    args = parser.parse_args()

    tmhm_moves = load_tmhm_list("src/item.c")
    species_dict = load_species_header("include/constants/species.h")
    moves_dict = load_moves_header("include/constants/moves.h")

    with open(args.learnsets, encoding="utf-8") as f:
        species_learnsets = json.load(f)

    if args.tmout:
        write_tm_data(species_dict, moves_dict, species_learnsets, tmhm_moves, args.tmout)

    if args.levelupout and args.levelupoffsetout:
        write_levelup_data(species_dict, moves_dict, species_learnsets, args.levelupout, args.levelupoffsetout)

    if args.eggout and args.eggoffsetout:
        write_eggmove_data(species_dict, moves_dict, species_learnsets, args.eggout, args.eggoffsetout)
